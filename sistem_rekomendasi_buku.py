# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi_Buku.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gwseoPmWq8ZfG2rExtFKPgfgC-EcM-13

# **Book Recommendation Dataset - Putri Sinta Dewi Sinaga**

## **Project Overview**

Buku adalah kumpulan/himpunan kertas atau lembaran yang tertulis atau mengandung tulisan. Setiap sisi dari sebuah lembaran kertas pada buku disebut sebuah halaman.Seiring dengan perkembangan dalam bidang dunia informatika, kini dikenal pula istilah e-book atau buku-e (buku elektronik) yang mengandalkan perangkat seperti komputer meja, komputer jinjing, komputer tablet, telepon seluler dan lainnya, serta menggunakan perangkat lunak tertentu untuk membacanya. Ada beberapa contoh buku yang membuat penikmat buku bingung akan buku yang ingin dibacanya. Dalam hal ini, sistem rekomendasi menjadi salah satu cara untuk memberi saran kepada penikmat buku tersebut.

# **Data Understanding**

Data yang digunakan untuk projek kali ini yaitu Book Recommendation Dataset yang diunduh dari kaggle. (https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset?select=Books.csv).

file yang terdapat pada dataset diatas adalah sebagai berikut:



*   Books.csv
*   Rating.cvs
*   users.cvs

## **Download dataset from Kaggle**
"""

# install kaggle package
!pip install -q kaggle

# upload kaggle.json
from google.colab import files
files.upload()

# make directory and change permission
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!ls ~/.kaggle

"""## **Data Loading**"""

# download dataset, choose 'copy api command' from kaggle dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset

# unzip
!mkdir book-recommendation-dataset.zip
!unzip book-recommendation-dataset.zip -d book-recommendation-dataset
!ls book-recommendation-dataset

"""## **Import library**"""

# Commented out IPython magic to ensure Python compatibility.
# Import library
import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
import keras
# %matplotlib inline
import seaborn as sns

from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from zipfile import ZipFile
from numpy import sqrt
from sklearn.metrics import mean_squared_error

# memuat file ke dalam variable
Books = pd.read_csv("/content/book-recommendation-dataset/Books.csv")
Ratings = pd.read_csv("/content/book-recommendation-dataset/Ratings.csv")
Users = pd.read_csv("/content/book-recommendation-dataset/Users.csv")

# menampilkan jumlah shape pada masing-masing file

print("Books Shape:", Books.shape)
print("Users Shape:", Users.shape)
print("Ratings Shape:", Ratings.shape)

"""## **Exploratory Data Analysis**

suatu proses uji investigasi awal yang bertujuan untuk mengidentifikasi pola, menemukan anomali, menguji hipotesis dan memeriksa asumsi.

### **Unvariate EDA**

Pada data loading, telah dideklarasikan variabel yang akan dipakai. Variable tersebut diantaranya :


1.   Books : Buku diidentifikasi dengan ISBN masing-masing. 
2.   Ratings : Berisi informasi penilaian buku
3.   Users : Berisi pengguna.





tahap eksplorasi dilakukan untuk memahami variable serta menemukan korelasi antar variable.

### **Books Variable**
"""

Books

#Cek informasi dari data Books
Books.info()

"""Berdasarkan hasil diatas terdapat empat variable diantaranya



*   Book Title : Judul Buku ada 271360
*   Book Author : Penulis Buku ada 271359
*   Year Of Publication : Tahun Penerbitan ada 271360
*   Publisher : Penerbit ada 271358
*   Image-URL-S: merupakan sampul gambar dari buku yang berukuran kecil dengan tipe data object
*   Image-URL-M: merupakan sampul gambar dari buku yang berukuran sedang dengan tipe data object
*   Image-URL-L: merupakan sampul gambar dari buku yang berukuran besar dengan tipe data object



"""

print('Banyak Buku :', len(Books))

data_books = Books.iloc[:12000]
print('Jumlah Buku yang di gunakan :', len(data_books))

"""### **Users Variable**"""

Users

#Cek informasi dari data Users
Users.info()

"""Berdasarkan hasil diatas terdapat empat variable diantaranya



*   User-ID : Pengguna ID ada 278858
*   Location : Lokasi ada 278858
*   Age  : Usia ada 168096
"""

print('Banyak Pengguna :', len(Users))

data_users = Users.iloc[:12000]
print('Jumlah pengguna yang di gunakan :', len(data_users))

"""### **Ratings Variable**"""

Ratings

#Cek informasi dari data Ratings
Ratings.info()

"""Berdasarkan hasil diatas terdapat satu variable diantaranya

*   User-ID : Pengguna ID ada 1149780
"""

print('Banyak Rating :', len(Ratings))

data_rating = Ratings.iloc[:12000]
print('Jumlah pengguna yang di gunakan :', len(data_rating))

"""## **Data Preparation**

Data preparation bertujuan untuk menyiapkan data sebelum masuk ke proses modeling. Selain itu, data preparation juga berguna untuk meningkatkan akurasi saat training data. Pada dataset ini, yang akan kita lakukan yaitu menggabungkan dataset dengan fungsi merge() dan key ISBN, menghapus missing value serta menurut dataset berdasarkan ISBN serta menghapus hasil duplikat.

### **Books Variable**
"""

data_books.rename(columns = {'Book-Title':'Book_Title', 'Book-Author': 'Book_Author', 'Year-Of-Publication': 'Year_Publication'},inplace = True)
data_books

data_books.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L' ], axis=1, inplace=True)
data_books

"""Menghapus beberapa kolom image yang tidak terlalu digunakan, yaitu :

1.  Image-URL-S : gambar dari buku yang berukuran kecil
2.  Image-URL-M : gambar dari buku yang berukuran sedang
3.  Image-URL-L : gambar dari buku yang berukuran besar
"""

data_books['Year_Publication'] = pd.to_numeric(data_books['Year_Publication'], errors='coerce')

data_books.info()

"""### **Ratings Variable**"""

data_rating.rename(columns = {'User-ID':'UserID', 'Book-Rating': 'Book_Rating'},inplace = True)
data_rating

"""### **Users Variable**"""

data_users.rename(columns = {'User-ID':'UserID',},inplace = True)
data_users

data_users.drop(['Age'], axis=1, inplace=True)
data_users

print('Jumlah Buku berdasarkan Rating : ', len(data_rating.ISBN.unique()))
print('Jumlah Buku berdasarkan Daftar Buku : ', len(data_books.ISBN.unique()))
print('Jumlah Pengguna berdasarkan ID PEngguna : ', len(data_users.UserID.unique()))

"""Menggabungkan 'data_books' dan 'data_rating' dengan menggunakan fungsi merge"""

data_train = data_rating.merge(data_books, left_on = 'ISBN', right_on = 'ISBN')
data_train

data_train.info()

year=data_train['Year_Publication'].value_counts()[0:10]
plt.figure(figsize=(16,8))
plt.title("10 Tahun terbanyak publikasi")
sns.barplot(x=year.index,y=year)
plt.xticks(rotation=90)
plt.xlabel('Year')
plt.ylabel('Count')
plt.show()

"""Menggabungkan 'data_user' dan 'data_rating' dengan menggunakan fungsi merge"""

data_using = data_rating.merge(data_users, left_on = 'UserID', right_on = 'UserID')
data_using

data_using.info()

most_user=data_using['UserID'].value_counts()[0:10]
plt.figure(figsize=(16,8))
plt.title("10 ID Pengguna terpopuler")
sns.barplot(x=year.index,y=year)
plt.xticks(rotation=90)
plt.xlabel('UserID')
plt.ylabel('Count')
plt.show()

most_author = data_train.Book_Author.value_counts().reset_index()
most_author.columns = ['Book_Author','count']

plt.figure(figsize = (16,8))
plt.title("10 Penulis Terpopuler")
sns.barplot(x = 'count', y = 'Book_Author', data = most_author.head(10), palette='icefire_r');
plt.ylabel('Book_Author')
plt.xlabel('Count')
plt.show()

most_loc = data_using.Location.value_counts().reset_index()
most_loc.columns = ['Location','count']

plt.figure(figsize = (16,8))
plt.title("10 Lokasi Penulis Terpopuler")
sns.barplot(x = 'count', y = 'Location', data = most_loc.head(10), palette='icefire_r');
plt.ylabel('Location')
plt.xlabel('Count')
plt.show()

most_publis = data_train.Publisher.value_counts().reset_index()
most_publis.columns = ['Publisher','count']

plt.figure(figsize = (16,8))
plt.title("10 Publisher terbaik")
sns.barplot(x = 'count', y = 'Publisher', data = most_publis.head(10));
plt.ylabel('Publisher')
plt.xlabel('Count')
plt.show()

data_aver = data_train.groupby('Book_Title', as_index=False)['Book_Rating'].mean()
temp = data_train.Book_Title.value_counts().reset_index()
temp.columns = ['Book_Title','count']
most_rated_by_reads = pd.merge(data_aver,temp,on='Book_Title')

most_rated_by_reads = most_rated_by_reads.sort_values('count',ascending=False)

plt.figure(figsize=(12,10))
plt.title("Rata-rata rating dengan buku terbanyak dibaca")
sns.barplot(x = 'Book_Rating', y = 'Book_Title', data = most_rated_by_reads.head(10), palette='icefire_r');

"""## **Data Cleaning**

proses mendeteksi dan mengoreksi (atau menghapus) catatan yang rusak atau tidak akurat dari kumpulan catatan, tabel, atau basis data dan mengacu pada pengidentifikasian bagian data yang tidak lengkap, salah, tidak akurat atau tidak relevan dan kemudian mengganti, memodifikasi, atau menghapus data kotor atau kasar.
"""

# check missing values
(data_books.isnull() | data_books.empty | data_books.isna()).sum()

# check missing values
(data_rating.isnull() | data_rating.empty | data_rating.isna()).sum()

# check missing values
(data_users.isnull() | data_users.empty | data_users.isna()).sum()

# check missing values
(data_train.isnull() | data_train.empty | data_train.isna()).sum()

# check missing values
(data_using.isnull() | data_using.empty | data_using.isna()).sum()

data_prep = data_train
data_prep.sort_values('ISBN')

data_prep = data_prep.drop_duplicates('ISBN')
data_prep

data_prus = data_using
data_prus.sort_values('UserID')

data_prus = data_prus.drop_duplicates('UserID')
data_prus

# Mengonversi data series 'ISBN’ menjadi dalam bentuk list
books_id = data_prep['ISBN'].tolist()
 
# Mengonversi data series ‘Title’ menjadi dalam bentuk list
books_title = data_prep['Book_Title'].tolist()
 
# Mengonversi data series ‘Author’ menjadi dalam bentuk list
books_author = data_prep['Book_Author'].tolist()
 
print('Jumlah ID Buku : ', len(books_id))
print('Jumlah Judul Buku : ', len(books_title))
print('Jumlah Penulis Buku : ', len(books_author))

# Membuat dictionary untuk data ‘books_id’, ‘books_title’, dan ‘books_author’
books_new = pd.DataFrame({
    'id': books_id,
    'title':books_title,
    'author': books_author
})
books_new

df = data_rating
df

# Mengubah UserID menjadi list tanpa nilai yang sama
user_ids = df['UserID'].unique().tolist()
print('list UserID: ', user_ids)
 
# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded UserID : ', user_to_user_encoded)
 
# Melakukan proses encoding angka ke UserID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke UserID: ', user_encoded_to_user)

# Mengubah ISBN menjadi list tanpa nilai yang sama
book_ids = df['ISBN'].unique().tolist()
 
# Melakukan proses encoding ISBN
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}
 
# Melakukan proses encoding angka ke ISBN
book_encoded_to_book = {i: x for i, x in enumerate(book_ids)}
 
#Selanjutnya, petakan userID dan ISBN ke dataframe yang berkaitan.
 
# Mapping userID ke dataframe user
df['user'] = df['UserID'].map(user_to_user_encoded)
 
# Mapping ISBN ke dataframe book
df['book'] = df['ISBN'].map(book_to_book_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah book
num_book = len(book_encoded_to_book)
print(num_book)
 
# Mengubah rating menjadi nilai float
df['Book_Rating'] = df['Book_Rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df['Book_Rating'])
 
# Nilai maksimal rating
max_rating = max(df['Book_Rating'])
 
print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['Book_Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 90% data train dan 20% data validasi
train_indices = int(0.9 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""## **Data Modelling**

proses untuk menjalankan algoritma Machine Learning untuk mengolah dataset yang sudah dibagi menjadi data training dan mengoptimalkan algoritma untuk menemukan pola atau output tertentu. Pada tahap ini saya menggunakan model collaborative filtering dimana menggunakan metode deep learning yang bertujuan menghasilkan rekomendasi buku.
"""

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_book = tf.tensordot(user_vector, book_vector, 2) 
 
    x = dot_user_book + user_bias + book_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training
 
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""## **Evaluasi**"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### **Sistem Collaborative Filtering**"""

book_df = books_new
 
# Mengambil sample user
user_id = df.UserID.sample(1).iloc[0]
book_visited_by_user = df[df.UserID == user_id]
 
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
book_not_visited = book_df[~book_df['id'].isin(book_visited_by_user.ISBN.values)]['id'] 
book_not_visited = list(
    set(book_not_visited)
    .intersection(set(book_to_book_encoded.keys()))
)
 
book_not_visited = [[book_to_book_encoded.get(x)] for x in book_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_visited), book_not_visited)
)

"""### **Hasil Sistem Rekomendasi Collaborative Filtering**"""

ratings = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Menampilkan Rekomendasi Untuk Pengguna: {}'.format(user_id))
print('===' * 9)
print('Buku dengan peringkat tinggi dari pengguna')
print('----' * 8)
 
top_book_user = (
    book_visited_by_user.sort_values(
        by = 'Book_Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
book_df_rows = book_df[book_df['id'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.title, ':', row.author)
 
print('----' * 8)
print('10 Rekomendasi Buku Teratas')
print('----' * 8)
 
recommended_book = book_df[book_df['id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.title, ':', row.author)

"""### **Mean Squared Error (MSE)**"""

print("MSE dari pada data train = ", mean_squared_error(y_true=y_train, y_pred=model.predict(x_train))/1e3)
print("MSE dari pada data validation = ", mean_squared_error(y_true=y_val, y_pred=model.predict(x_val))/1e3)

